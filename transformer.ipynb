{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from typing import Any, Optional, Callable, Sequence\n",
    "\n",
    "from absl import logging\n",
    "import flax.linen as nn\n",
    "from flax.linen.linear import default_kernel_init\n",
    "from immutabledict import immutabledict\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "from scenic.common_lib import video_utils\n",
    "from scenic.model_lib.base_models import base_model\n",
    "from scenic.model_lib.base_models import classification_model\n",
    "from scenic.model_lib.base_models import model_utils as base_model_utils\n",
    "from scenic.model_lib.base_models.classification_model import ClassificationModel\n",
    "from scenic.model_lib.layers import attention_layers\n",
    "from scenic.model_lib.layers import nn_layers\n",
    "from scenic.projects.baselines import vit\n",
    "from scenic.projects.vivit import model_utils\n",
    "Initializer = Callable[[jnp.ndarray, Sequence[int], jnp.dtype], jnp.ndarray]\n",
    "_AXIS_TO_NAME = immutabledict({\n",
    "    1: 'time',\n",
    "    2: 'space',\n",
    "})\n",
    "\n",
    "KERNEL_INITIALIZERS = immutabledict({\n",
    "    'zero': nn.initializers.zeros,\n",
    "    'xavier': nn.initializers.xavier_uniform(),\n",
    "})\n",
    "ViViT_CLASSIFICATION_METRICS_BASIC = immutabledict({\n",
    "    'accuracy': (base_model_utils.weighted_correctly_classified,\n",
    "                 base_model_utils.num_examples),\n",
    "    'loss': (base_model_utils.weighted_unnormalized_softmax_cross_entropy,\n",
    "             base_model_utils.num_examples)\n",
    "})\n",
    "\n",
    "ViViT_CLASSIFICATION_METRICS = immutabledict({\n",
    "    **ViViT_CLASSIFICATION_METRICS_BASIC,\n",
    "    'accuracy_top_5': (functools.partial(\n",
    "        base_model_utils.weighted_topk_correctly_classified,\n",
    "        k=5), base_model_utils.num_examples),\n",
    "})\n",
    "\n",
    "\n",
    "def _reshape_to_time_space(x, temporal_dims):\n",
    "  if x.ndim == 3:\n",
    "    b, thw, d = x.shape\n",
    "    assert thw % temporal_dims == 0\n",
    "    hw = thw // temporal_dims\n",
    "    x = jnp.reshape(x, [b, temporal_dims, hw, d])\n",
    "  assert x.ndim == 4\n",
    "  return x\n",
    "\n",
    "\n",
    "def embed_2d_patch(x, patches, embedding_dim):\n",
    "  \"\"\"Standard ViT method of embedding input patches.\"\"\"\n",
    "\n",
    "  n, h, w, c = x.shape\n",
    "\n",
    "  assert patches.get('size') is not None, ('patches.size is now the only way'\n",
    "                                           'to define the patches')\n",
    "\n",
    "  fh, fw = patches.size\n",
    "  gh, gw = h // fh, w // fw\n",
    "\n",
    "  if embedding_dim:\n",
    "    x = nn.Conv(\n",
    "        embedding_dim, (fh, fw),\n",
    "        strides=(fh, fw),\n",
    "        padding='VALID',\n",
    "        name='embedding')(x)\n",
    "  else:\n",
    "    # This path often results in excessive padding: b/165788633\n",
    "    x = jnp.reshape(x, [n, gh, fh, gw, fw, c])\n",
    "    x = jnp.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "    x = jnp.reshape(x, [n, gh, gw, -1])\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "def embed_3d_patch(x,\n",
    "                   patches,\n",
    "                   embedding_dim,\n",
    "                   kernel_init_method,\n",
    "                   name='embedding'):\n",
    "  \"\"\"Embed 3D input patches into tokens.\"\"\"\n",
    "\n",
    "  assert patches.get('size') is not None, 'patches.size must be defined'\n",
    "  assert len(patches.size) == 3, 'patches.size must have 3 elements'\n",
    "  assert embedding_dim, 'embedding_dim must be specified'\n",
    "\n",
    "  fh, fw, ft = patches.size\n",
    "\n",
    "  if kernel_init_method == 'central_frame_initializer':\n",
    "    kernel_initializer = model_utils.central_frame_initializer()\n",
    "    logging.info('Using central frame initializer for input embedding')\n",
    "  elif kernel_init_method == 'average_frame_initializer':\n",
    "    kernel_initializer = model_utils.average_frame_initializer()\n",
    "    logging.info('Using average frame initializer for input embedding')\n",
    "  else:\n",
    "    kernel_initializer = default_kernel_init\n",
    "    logging.info('Using default initializer for input embedding')\n",
    "\n",
    "  x = nn.Conv(\n",
    "      embedding_dim, (ft, fh, fw),\n",
    "      strides=(ft, fh, fw),\n",
    "      padding='VALID',\n",
    "      name=name,\n",
    "      kernel_init=kernel_initializer)(\n",
    "          x)\n",
    "\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_encode(x,\n",
    "                    #temporal_encoding_config,\n",
    "                    patches,\n",
    "                    hidden_size,\n",
    "                    return_1d=True,\n",
    "                    name='embedding'):\n",
    "  \"\"\"Encode video for feeding into ViT.\"\"\"\n",
    "\n",
    "  n, _, in_h, in_w, c = x.shape\n",
    "\n",
    "  # if temporal_encoding_config.method == 'temporal_sampling':\n",
    "  # n_sampled_frames = 16\n",
    "  # x = video_utils.sample_frames_uniformly(x, n_sampled_frames)\n",
    "  # t_s = x.shape[1]\n",
    "  # x = jnp.reshape(x, [n, t_s * in_h, in_w, c])\n",
    "\n",
    "  #   x = embed_2d_patch(x, patches, hidden_size)\n",
    "  #   temporal_dims = t_s\n",
    "  #   if return_1d:\n",
    "  #     n, th, w, c = x.shape\n",
    "  #     x = jnp.reshape(x, [n, th * w, c])\n",
    "  #   else:\n",
    "  #     n, th, w, c = x.shape\n",
    "  #     x = jnp.reshape(x, [n, t_s, -1, w, c])\n",
    "\n",
    " \n",
    "  kernel_init_method = 'central_frame_initializer'\n",
    "  x = embed_3d_patch(x, patches, hidden_size, kernel_init_method, name)\n",
    "  temporal_dims = x.shape[1]\n",
    "  if return_1d:\n",
    "    n, t, h, w, c = x.shape\n",
    "    x = np.reshape(x, [n, t * h * w, c])\n",
    "\n",
    "\n",
    "  assert x.size > 0, ('Found zero tokens after temporal encoding. '\n",
    "                      'Perhaps one of the patch sizes is such that '\n",
    "                      'floor(dim_size / patch_size) = 0?')\n",
    "\n",
    "  return x, temporal_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderFactorizedSelfAttentionBlock(nn.Module):\n",
    "  \"\"\"Encoder with facctorized self attention block.\n",
    "\n",
    "  Attributes:\n",
    "    mlp_dim: Dimension of the mlp on top of attention block.\n",
    "    num_heads: Number of heads.\n",
    "    temporal_dims: Number of temporal dimensions in the flattened input\n",
    "    attention_kernel_initializer: Initializer to use for attention layers.\n",
    "    dropout_rate: Dropout rate.\n",
    "    attention_dropout_rate: Dropout for attention heads.\n",
    "    droplayer_p: Probability of dropping a layer.\n",
    "    attention_order: The order to do the attention. Choice of {time_space,\n",
    "      space_time}.\n",
    "    dtype: the dtype of the computation (default: float32).\n",
    "  \"\"\"\n",
    "  mlp_dim: int\n",
    "  num_heads: int\n",
    "  temporal_dims: int\n",
    "  attention_kernel_initializer: Initializer\n",
    "  dropout_rate: float = 0.1\n",
    "  attention_dropout_rate: float = 0.1\n",
    "  droplayer_p: Optional[float] = None\n",
    "  attention_order: str = 'time_space'\n",
    "  dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs: jnp.ndarray, *, deterministic: bool=False):\n",
    "    \"\"\"Applies Encoder1DBlock module.\"\"\"\n",
    "    some_param = self.param('some_param', nn.initializers.zeros_init(), (1, ))\n",
    "    dropout_rng = self.make_rng('dropout')\n",
    "\n",
    "    b, thw, d = inputs.shape\n",
    "    inputs = _reshape_to_time_space(inputs, self.temporal_dims)\n",
    "    # self_attention = functools.partial(\n",
    "    #     nn.SelfAttention,\n",
    "    #     num_heads=self.num_heads,\n",
    "    #     kernel_init=self.attention_kernel_initializer,\n",
    "    #     broadcast_dropout=False,\n",
    "    #     dropout_rate=self.attention_dropout_rate,\n",
    "    #     dtype=self.dtype)\n",
    "\n",
    "    if self.attention_order == 'time_space':\n",
    "      attention_axes = (1, 2)\n",
    "    elif self.attention_order == 'space_time':\n",
    "      attention_axes = (2, 1)\n",
    "    else:\n",
    "      raise ValueError(f'Invalid attention order {self.attention_order}.')\n",
    "\n",
    "    def _run_attention_on_axis(inputs, axis, two_d_shape):\n",
    "      \"\"\"Reshapes the input and run attention on the given axis.\"\"\"\n",
    "      inputs = model_utils.reshape_to_1d_factorized(inputs, axis=axis)\n",
    "      x = nn.LayerNorm(\n",
    "          dtype=self.dtype, name='LayerNorm_{}'.format(_AXIS_TO_NAME[axis]))(\n",
    "              inputs)\n",
    "      output = nn.SelfAttention(num_heads=self.num_heads,\n",
    "        kernel_init=self.attention_kernel_initializer,\n",
    "        broadcast_dropout=False,\n",
    "        dropout_rate=self.attention_dropout_rate,\n",
    "        dtype=self.dtype,\n",
    "        deterministic=deterministic,\n",
    "        name='MultiHeadDotProductAttention_{}'.format(_AXIS_TO_NAME[axis]))\n",
    "      init_rngs = {'params': jax.random.key(0), 'dropout': jax.random.key(1)}\n",
    "\n",
    "      variables = output.init(init_rngs, x, method=nn.SelfAttention.__call__)\n",
    "\n",
    "      x=output.apply(variables,x,rngs={'dropout': jax.random.key(2)})\n",
    "      \n",
    "      x = nn.Dropout(rate=self.dropout_rate)(x, deterministic)\n",
    "      x = x + inputs\n",
    "      return model_utils.reshape_to_2d_factorized(\n",
    "          x, axis=axis, two_d_shape=two_d_shape)\n",
    "\n",
    "    x = inputs\n",
    "    two_d_shape = inputs.shape\n",
    "    for axis in attention_axes:\n",
    "      x = _run_attention_on_axis(x, axis, two_d_shape)\n",
    "\n",
    "    # MLP block.\n",
    "    x = jnp.reshape(x, [b, thw, d])\n",
    "    y = nn.LayerNorm(dtype=self.dtype, name='LayerNorm_mlp')(x)\n",
    "    op = attention_layers.MlpBlock(\n",
    "        mlp_dim=self.mlp_dim,\n",
    "        dtype=self.dtype,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        activation_fn=nn.gelu,\n",
    "        kernel_init=nn.initializers.xavier_uniform(),\n",
    "        bias_init=nn.initializers.normal(stddev=1e-6),\n",
    "        name='MlpBlock')\n",
    "    init_rngs = {'params': jax.random.key(0), 'dropout': jax.random.key(1)}\n",
    "\n",
    "    variables = op.init(init_rngs, y)\n",
    "\n",
    "    y=op.apply(variables,y,rngs={'dropout': jax.random.key(2)})\n",
    "    \n",
    "    return x + y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  \"\"\"Transformer Encoder.\n",
    "\n",
    "  Attributes:\n",
    "    inputs: nd-array, Input data\n",
    "    temporal_dims: Number of temporal dimensions in the input.\n",
    "    mlp_dim: Dimension of the mlp on top of attention block.\n",
    "    num_layers: Number of layers.\n",
    "    num_heads: Number of attention heads.\n",
    "    attention_config: Has parameters for the type of attention.\n",
    "    dropout_rate: Dropout rate.\n",
    "    attention_dropout_rate: Dropout for attention heads.\n",
    "    stochastic_droplayer_rate: Probability of dropping a layer linearly\n",
    "      grows from 0 to the provided value. Our implementation of stochastic\n",
    "      depth follows timm library, which does per-example layer dropping and\n",
    "      uses independent dropping patterns for each skip-connection.\n",
    "    positional_embedding: The type of positional embedding to use. Supported\n",
    "      values are {learned_1d, sinusoidal_1d, sinusoidal_3d, none}.\n",
    "    normalise_output: If True, perform layernorm on the output.\n",
    "  \"\"\"\n",
    "\n",
    "  temporal_dims: Optional[int]\n",
    "  mlp_dim: int\n",
    "  num_layers: int\n",
    "  num_heads: int\n",
    "  #attention_config: ml_collections.ConfigDict = None\n",
    "  dropout_rate: float = 0.1\n",
    "  attention_dropout_rate: float = 0.1\n",
    "  stochastic_droplayer_rate: float = 0.0\n",
    "  dtype: jnp.dtype = jnp.float32\n",
    "  positional_embedding: str = 'learned_1d'\n",
    "  normalise_output: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs: jnp.ndarray, *, train: bool = True):\n",
    "    \"\"\"Applies Transformer model on the inputs.\"\"\"\n",
    "    # some_param = self.param('some_param', nn.initializers.zeros_init(), (1, ))\n",
    "    # dropout_rng = self.make_rng('dropout')\n",
    "    assert inputs.ndim == 3  # (batch, len, emb)\n",
    "    dtype = jax.dtypes.canonicalize_dtype(self.dtype)\n",
    "\n",
    "    if self.positional_embedding == 'learned_1d':\n",
    "      x = vit.AddPositionEmbs(\n",
    "          posemb_init=nn.initializers.normal(stddev=0.02),  # from BERT.\n",
    "          name='posembed_input')(inputs)\n",
    "    # elif self.positional_embedding == 'sinusoidal_1d':\n",
    "    #   x = attention_layers.Add1DPositionEmbedding(\n",
    "    #       posemb_init=None)(inputs)\n",
    "    # elif self.positional_embedding == 'sinusoidal_3d':\n",
    "    #   batch, num_tokens, hidden_dim = inputs.shape\n",
    "    #   height = width = int(np.sqrt(num_tokens // self.temporal_dims))\n",
    "    #   if height * width * self.temporal_dims != num_tokens:\n",
    "    #     raise ValueError('Input is assumed to be square for sinusoidal init.')\n",
    "    #   inputs_reshape = inputs.reshape([batch, self.temporal_dims, height, width,\n",
    "    #                                    hidden_dim])\n",
    "    #   x = attention_layers.AddFixedSinCosPositionEmbedding()(inputs_reshape)\n",
    "    #   x = x.reshape([batch, num_tokens, hidden_dim])\n",
    "    # elif self.positional_embedding == 'none':\n",
    "    #   x = inputs\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          f'Unknown positional embedding {self.positional_embedding}')\n",
    "    # x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=not train)\n",
    "\n",
    "    # if self.attention_config is None or self.attention_config.type in [\n",
    "    #     'spacetime', 'factorized_encoder'\n",
    "    # ]:\n",
    "    #   encoder_block = EncoderBlock\n",
    "    #elif self.attention_config.type == 'factorized_self_attention_block':\n",
    "    encoder_block = functools.partial(\n",
    "          EncoderFactorizedSelfAttentionBlock,\n",
    "          attention_order='space_time',\n",
    "          attention_kernel_initializer=KERNEL_INITIALIZERS[('xavier')],\n",
    "          temporal_dims=self.temporal_dims)\n",
    "    \n",
    "    # elif self.attention_config.type == 'factorized_dot_product_attention':\n",
    "    # b, thw, d = x.shape\n",
    "    # x = _reshape_to_time_space(x, self.temporal_dims)  # [b, t, hw, d]\n",
    "    # encoder_block = functools.partial(\n",
    "    #       EncoderBlock,\n",
    "    #       attention_fn=functools.partial(\n",
    "    #           model_utils.factorized_dot_product_attention))\n",
    "    # # else:\n",
    "    #   raise ValueError(f'Unknown attention type {self.attention_config.type}')\n",
    "\n",
    "    # Input Encoder\n",
    "    for lyr in range(self.num_layers):\n",
    "      droplayer_p = (\n",
    "          lyr / max(self.num_layers - 1, 1)) * self.stochastic_droplayer_rate\n",
    "      output = encoder_block(\n",
    "          mlp_dim=self.mlp_dim,\n",
    "          num_heads=self.num_heads,\n",
    "          dropout_rate=self.dropout_rate,\n",
    "          attention_dropout_rate=self.attention_dropout_rate,\n",
    "          droplayer_p=droplayer_p,\n",
    "          name=f'encoderblock_{lyr}',\n",
    "          dtype=dtype)\n",
    "      init_rngs = {'params': jax.random.key(0), 'dropout': jax.random.key(1)}\n",
    "\n",
    "      variables = output.init(init_rngs, x, method=EncoderFactorizedSelfAttentionBlock.__call__)\n",
    "\n",
    "      x=output.apply(variables,x,rngs={'dropout': jax.random.key(2)})\n",
    "\n",
    "    # if self.attention_config.type == 'factorized_dot_product_attention':\n",
    "    #   # Reshape back to 3D:\n",
    "    #   x = jnp.reshape(x, [b, thw, d])\n",
    "\n",
    "    if self.normalise_output:\n",
    "      encoded = nn.LayerNorm(name='encoder_norm')(x)\n",
    "    else:\n",
    "      encoded = x\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViViT(nn.Module):\n",
    "  \"\"\"Vision Transformer model for Video.\n",
    "\n",
    "  Attributes:\n",
    "    mlp_dim: Dimension of the mlp on top of attention block.\n",
    "    num_classes: Number of output classes.\n",
    "    num_heads: Number of self-attention heads.\n",
    "    num_layers: Number of layers.\n",
    "    patches: Configuration of the patches extracted in the stem of the model.\n",
    "    hidden_size: Size of the hidden state of the output of model's stem.\n",
    "    representation_size: Size of the representation layer in the model's head.\n",
    "      if None, we skip the extra projection + tanh activation at the end.\n",
    "    temporal_encoding_config: ConfigDict which defines the type of input\n",
    "      encoding when tokenising the video.\n",
    "    attention_config: ConfigDict which defines the type of spatio-temporal\n",
    "      attention applied in the model.\n",
    "    dropout_rate: Dropout rate.\n",
    "    attention_dropout_rate: Dropout for attention heads.\n",
    "    stochastic_droplayer_rate: Probability of dropping a layer. Linearly\n",
    "      increases from 0 to the provided value..\n",
    "    classifier: type of the classifier layer. Options are 'gap', 'gmp', 'gsp',\n",
    "      'token'.\n",
    "    return_prelogits: If true, return the final representation of the network\n",
    "      before the classification head. Useful when using features for a\n",
    "      downstream task.\n",
    "    return_preclassifier: If true, return the representation after the\n",
    "      transformer encoder. Useful if using this as the backbone stem as part\n",
    "      of a bigger architecture.\n",
    "    dtype: JAX data type for activations.\n",
    "  \"\"\"\n",
    "\n",
    "  mlp_dim: int\n",
    "  num_layers: int\n",
    "  num_heads: int\n",
    "  # num_classes: int\n",
    "  patches: ml_collections.ConfigDict\n",
    "  hidden_size: int\n",
    "  #temporal_encoding_config: ml_collections.ConfigDict\n",
    "  #attention_config: ml_collections.ConfigDict\n",
    "  representation_size: Optional[int] = None\n",
    "  dropout_rate: float = 0.1\n",
    "  attention_dropout_rate: float = 0.1\n",
    "  stochastic_droplayer_rate: float = 0.\n",
    "  classifier: str = 'gap'\n",
    "  #return_prelogits: bool = True\n",
    "  #return_preclassifier: bool = False\n",
    "  dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, *, train: bool = True, debug: bool = False):\n",
    "\n",
    "   \n",
    "\n",
    "    x1, temporal_dims = temporal_encode(\n",
    "        x, self.patches, self.hidden_size)\n",
    "\n",
    "    # # If we want to add a class token, add it here.\n",
    "    # if self.classifier in ['token']:\n",
    "    #   n, _, c = x.shape\n",
    "    #   cls = self.param('cls', nn.initializers.zeros, (1, 1, c), x.dtype)\n",
    "    #   cls = jnp.tile(cls, [n, 1, 1])\n",
    "    #   x = jnp.concatenate([cls, x], axis=1)\n",
    "\n",
    "    \n",
    "    output = Encoder(\n",
    "        temporal_dims=temporal_dims,\n",
    "        mlp_dim=self.mlp_dim,\n",
    "        num_layers=self.num_layers,\n",
    "        num_heads=self.num_heads,\n",
    "        \n",
    "        dropout_rate=self.dropout_rate,\n",
    "        attention_dropout_rate=self.attention_dropout_rate,\n",
    "        stochastic_droplayer_rate=self.stochastic_droplayer_rate,\n",
    "        dtype=self.dtype,\n",
    "        name='Transformer')\n",
    "    init_rngs = {'params': jax.random.key(0), 'dropout': jax.random.key(1)}\n",
    "\n",
    "    variables = output.init(init_rngs, x1)\n",
    "\n",
    "    x2=output.apply(variables,x1,rngs={'dropout': jax.random.key(2)})\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # if self.return_preclassifier:\n",
    "    #   return x\n",
    "\n",
    "    #if self.classifier in ['token', '0']:\n",
    "    x2 = x2[:, 0]\n",
    "    # elif self.classifier in ('gap', 'gmp', 'gsp'):\n",
    "    #   fn = {'gap': jnp.mean, 'gmp': jnp.max, 'gsp': jnp.sum}[self.classifier]\n",
    "    #   x = fn(x, axis=list(range(1, x.ndim - 1)))\n",
    "\n",
    "    if self.representation_size is not None:\n",
    "      x2 = nn.Dense(self.representation_size, name='pre_logits')(x)\n",
    "      x2 = nn.tanh(x)\n",
    "    else:\n",
    "      x2 = nn_layers.IdentityLayer(name='pre_logits')(x)\n",
    "\n",
    "    \n",
    "    return x2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frames tensor shape: (1, 781, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_video(video_path, output_fps, min_resize=256, crop_size=224, zero_centering=True):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate frame interval to extract 16 frames per second\n",
    "    interval = int(round(fps / output_fps))\n",
    "\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frames.append(frame)\n",
    "        for _ in range(interval - 1):\n",
    "            cap.grab()  # Move to the next frame without decoding\n",
    "\n",
    "    # Resize frames to min_resize and crop to crop_size\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        resized_frame = cv2.resize(frame, (min_resize, min_resize))\n",
    "        cropped_frame = resized_frame[(min_resize - crop_size) // 2:(min_resize + crop_size) // 2,\n",
    "                                      (min_resize - crop_size) // 2:(min_resize + crop_size) // 2]\n",
    "\n",
    "        if zero_centering:\n",
    "            # Normalize pixel values to [-1, 1]\n",
    "            cropped_frame = cropped_frame.astype(float) / 255.0  # Assuming original range [0, 255]\n",
    "            cropped_frame = (cropped_frame - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "\n",
    "        processed_frames.append(cropped_frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Convert frames to a tensor (NumPy array)\n",
    "    frames_tensor = np.array(processed_frames)\n",
    "\n",
    "    # Reshape tensor to match the desired output shape\n",
    "    num_frames = len(processed_frames)\n",
    "    frames_tensor = frames_tensor.reshape(1, num_frames, crop_size, crop_size, 3)\n",
    "\n",
    "    return frames_tensor\n",
    "\n",
    "# Example usage:\n",
    "video_path = r\"E:\\train\\1-1004\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.mp4\"\n",
    "output_fps = 16\n",
    "processed_frames_tensor = preprocess_video(video_path, output_fps)\n",
    "print(f\"Processed frames tensor shape: {processed_frames_tensor.shape}\")\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Convert NumPy array to JAX NumPy array\n",
    "jax_array = jnp.array(processed_frames_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "import haiku as hk\n",
    "import jax\n",
    "\n",
    "\n",
    "\n",
    "ipatches = ml_collections.ConfigDict()\n",
    "ipatches.size = (16, 16, 2)\n",
    "params = ipatches\n",
    "output=ViViT(\n",
    "          \n",
    "          \n",
    "          #num_classes=self.dataset_meta_data['num_classes'],\n",
    "          mlp_dim=3072,\n",
    "          num_layers=12,\n",
    "          num_heads=12,\n",
    "          representation_size=None,\n",
    "          patches=ipatches,\n",
    "          hidden_size=768,\n",
    "          #temporal_encoding_config=self.config.model.temporal_encoding_config,\n",
    "          #attention_config=self.config.model.attention_config,\n",
    "          classifier='token',\n",
    "          dropout_rate=0.1,\n",
    "          attention_dropout_rate=0.1,\n",
    "          stochastic_droplayer_rate= 0,\n",
    "          #return_prelogits=self.config.model.get('return_prelogits', False),\n",
    "          #return_preclassifier=self.config.model.get(\n",
    "          #    'return_preclassifier', False),\n",
    "\n",
    "          dtype=jnp.float32\n",
    "      )\n",
    "\n",
    "init_rngs = {'params': jax.random.key(0), 'dropout': jax.random.key(1)}\n",
    "\n",
    "variables = output.init(init_rngs, jax_array, method=ViViT.__call__)\n",
    "\n",
    "foutput=output.apply(variables,jax_array,rngs={'dropout': jax.random.key(2)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(r'E:\\train\\1-1004\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.npy',foutput)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
