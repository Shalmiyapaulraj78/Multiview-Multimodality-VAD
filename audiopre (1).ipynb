{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(Module):\n",
    "    r\"\"\"Encoder is a stack of N encoder layers\n",
    "\n",
    "    Args:\n",
    "        encoder_layer: an instance of the EncoderLayer() class (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src):\n",
    "        r\"\"\"Pass the input through the endocder layers in turn.\n",
    "\n",
    "        \"\"\"\n",
    "        output = src\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            output = self.layers[i](output)\n",
    "\n",
    "        if self.norm:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(Module):\n",
    "    r\"\"\"Decoder is a stack of N decoder layers\n",
    "\n",
    "    Args:\n",
    "        decoder_layer: an instance of the DecoderLayer() class (required).\n",
    "        num_layers: the number of sub-decoder-layers in the decoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n",
    "        \"\"\"\n",
    "        output = tgt\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            output = self.layers[i](output, memory)\n",
    "\n",
    "        if self.norm:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class EncoderLayer(Module):\n",
    "    r\"\"\"EncoderLayer is mainly made up of self-attention.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, src):\n",
    "        r\"\"\"Pass the input through the endocder layer.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        if hasattr(self, \"activation\"):\n",
    "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        else:  # for backward compatibility\n",
    "            src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "\n",
    "class DecoderLayer(Module):\n",
    "    r\"\"\"DecoderLayer is mainly made up of the proposed cross-modal relation attention (CMRA).\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "        \"\"\"\n",
    "        memory = torch.cat([memory, tgt], dim=0)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        if hasattr(self, \"activation\"):\n",
    "            tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        else:  # for backward compatibility\n",
    "            tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    else:\n",
    "        raise RuntimeError(\"activation should be relu/gelu, not %s.\" % activation)\n",
    "\n",
    "\n",
    "class New_Audio_Guided_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(New_Audio_Guided_Attention, self).__init__()\n",
    "        self.hidden_size = 1024\n",
    "        self.relu = nn.ReLU()\n",
    "        # channel attention\n",
    "        self.affine_video_1 = nn.Linear(1024, 1024)\n",
    "        self.affine_audio_1 = nn.Linear(128, 1024)\n",
    "        self.affine_bottleneck = nn.Linear(1024, 256)\n",
    "        self.affine_v_c_att = nn.Linear(256, 1024)\n",
    "        # spatial attention\n",
    "        self.affine_video_2 = nn.Linear(1024, 256)\n",
    "        self.affine_audio_2 = nn.Linear(128, 256)\n",
    "        self.affine_v_s_att = nn.Linear(256, 1)\n",
    "\n",
    "        # video-guided audio attention\n",
    "        self.affine_video_guided_1 = nn.Linear(1024, 64)\n",
    "        self.affine_video_guided_2 = nn.Linear(64, 128)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, video, audio):\n",
    "        '''\n",
    "        :param visual_feature: [batch, 10, 7, 7, 512]\n",
    "        :param audio_feature:  [batch, 10, 128]\n",
    "        :return: [batch, 10, 512]\n",
    "        '''\n",
    "        audio = audio.transpose(1, 0)\n",
    "        batch, t_size, f, v_dim = video.size()\n",
    "        a_dim = audio.size(-1)\n",
    "        audio_feature = audio.reshape(batch * t_size, a_dim)\n",
    "        visual_feature = video.reshape(batch, t_size, f)\n",
    "        raw_visual_feature = video\n",
    "        print(visual_feature.shape)\n",
    "        print(audio_feature.shape)\n",
    "\n",
    "\n",
    "        audio_query_1 = self.relu(self.affine_audio_1(audio_feature))\n",
    "        # ============================== Channel Attention ====================================\n",
    "\n",
    "        video_query_1 = self.relu(self.affine_video_1(visual_feature))\n",
    "\n",
    "        video_query_1 = video_query_1.reshape(batch*t_size, f)\n",
    "        print(audio_query_1.shape)\n",
    "        print(video_query_1.shape)\n",
    "\n",
    "        audio_video_query_raw = (audio_query_1 * video_query_1)\n",
    "        audio_video_query = self.relu(self.affine_bottleneck(audio_video_query_raw))\n",
    "        channel_att_maps = self.affine_v_c_att(audio_video_query).sigmoid().reshape(batch, t_size, -1,v_dim)\n",
    "        print(channel_att_maps.shape)\n",
    "        c_att_visual_feat = (raw_visual_feature * (channel_att_maps))\n",
    "\n",
    "        # ============================== Spatial Attention =====================================\n",
    "        # channel attended visual feature: [batch * 10, 49, v_dim]\n",
    "        c_att_visual_feat = c_att_visual_feat.reshape(batch*t_size, v_dim, -1)\n",
    "        c_att_visual_query = self.relu(self.affine_video_2(c_att_visual_feat))\n",
    "        audio_query_2 = self.relu(self.affine_audio_2(audio_feature)).unsqueeze(-2)\n",
    "        audio_video_query_2 = c_att_visual_query * audio_query_2\n",
    "        spatial_att_maps = self.softmax(self.tanh(self.affine_v_s_att(audio_video_query_2)).transpose(2, 1))\n",
    "        c_s_att_visual_feat = torch.bmm(spatial_att_maps, c_att_visual_feat).squeeze().reshape(batch, t_size, f)\n",
    "\n",
    "        return c_s_att_visual_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "\n",
    "class InternalTemporalRelationModule(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super(InternalTemporalRelationModule, self).__init__()\n",
    "        self.encoder_layer = EncoderLayer(d_model=d_model, nhead=4)\n",
    "        self.encoder = Encoder(self.encoder_layer, num_layers=2)\n",
    "\n",
    "        self.affine_matrix = nn.Linear(input_dim, d_model)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # add relu here?\n",
    "\n",
    "    def forward(self, feature):\n",
    "        # feature: [seq_len, batch, dim]\n",
    "        feature = self.affine_matrix(feature)\n",
    "        feature = self.encoder(feature)\n",
    "\n",
    "        return feature\n",
    "\n",
    "\n",
    "class CrossModalRelationAttModule(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super(CrossModalRelationAttModule, self).__init__()\n",
    "\n",
    "        self.decoder_layer = DecoderLayer(d_model=d_model, nhead=4)\n",
    "        self.decoder = Decoder(self.decoder_layer, num_layers=1)\n",
    "\n",
    "        self.affine_matrix = nn.Linear(input_dim, d_model)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, query_feature, memory_feature):\n",
    "        query_feature = self.affine_matrix(query_feature)\n",
    "        output = self.decoder(query_feature, memory_feature)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class WeaklyLocalizationModule(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(WeaklyLocalizationModule, self).__init__()\n",
    "\n",
    "        self.hidden_dim = input_dim # need to equal d_model\n",
    "        self.classifier = nn.Linear(self.hidden_dim, 1) # start and end\n",
    "        self.event_classifier = nn.Linear(self.hidden_dim, 29)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, fused_content):\n",
    "        fused_content = fused_content.transpose(0, 1)\n",
    "        max_fused_content, _ = fused_content.max(1)\n",
    "        # confident scores\n",
    "        is_event_scores = self.classifier(fused_content)\n",
    "        # classification scores\n",
    "        raw_logits = self.event_classifier(max_fused_content)[:, None, :]\n",
    "        # fused\n",
    "        fused_logits = is_event_scores.sigmoid() * raw_logits\n",
    "        # Training: max pooling for adapting labels\n",
    "        logits, _ = torch.max(fused_logits, dim=1)\n",
    "        event_scores = self.softmax(logits)\n",
    "\n",
    "        return is_event_scores.squeeze(), raw_logits.squeeze(), event_scores\n",
    "\n",
    "\n",
    "class SupvLocalizeModule(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SupvLocalizeModule, self).__init__()\n",
    "        # self.affine_concat = nn.Linear(2*256, 256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.classifier = nn.Linear(d_model, 1) # start and end\n",
    "        self.event_classifier = nn.Linear(d_model, 28)\n",
    "       # self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, fused_content):\n",
    "\n",
    "        max_fused_content, _ = fused_content.transpose(1, 0).max(1)\n",
    "        logits = self.classifier(fused_content)\n",
    "        # scores = self.softmax(logits)\n",
    "        class_logits = self.event_classifier(max_fused_content)\n",
    "        class_scores = class_logits\n",
    "\n",
    "        return logits, class_scores\n",
    "\n",
    "\n",
    "# class AudioVideoInter(nn.Module):\n",
    "#     def __init__(self, d_model, n_head, head_dropout=0.1):\n",
    "#         super(AudioVideoInter, self).__init__()\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.video_multihead = MultiheadAttention(d_model, num_heads=n_head, dropout=head_dropout)\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "#     def forward(self, video_feat, audio_feat):\n",
    "#         # video_feat, audio_feat: [10, batch, 256]\n",
    "#         global_feat = video_feat * audio_feat\n",
    "#         memory = torch.cat([audio_feat, video_feat], dim=0)\n",
    "#         mid_out = self.video_multihead(global_feat, memory, memory)[0]\n",
    "#         output = self.norm1(global_feat + self.dropout(mid_out))\n",
    "\n",
    "#         return  output\n",
    "\n",
    "\n",
    "class weak_main_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(weak_main_model, self).__init__()\n",
    "        self.spatial_channel_att = New_Audio_Guided_Attention().cuda()\n",
    "        self.video_input_dim = 512 \n",
    "        self.video_fc_dim = 512\n",
    "        self.d_model = 256\n",
    "        self.v_fc = nn.Linear(self.video_input_dim, self.video_fc_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.video_encoder = InternalTemporalRelationModule(input_dim=self.video_fc_dim, d_model=self.d_model)\n",
    "        self.video_decoder = CrossModalRelationAttModule(input_dim=self.video_fc_dim, d_model=self.d_model)\n",
    "        self.audio_encoder = InternalTemporalRelationModule(input_dim=128, d_model=self.d_model)\n",
    "        self.audio_decoder = CrossModalRelationAttModule(input_dim=128, d_model=self.d_model)\n",
    "\n",
    "        self.AVInter = AudioVideoInter(self.d_model, n_head=2, head_dropout=0.1)\n",
    "        self.localize_module = WeaklyLocalizationModule(self.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, visual_feature, audio_feature):\n",
    "        # [batch, 10, 512]\n",
    "        # this fc is optinal, that is used for adaption of different visual features (e.g., vgg, resnet).\n",
    "        audio_feature = audio_feature.transpose(1, 0).contiguous()\n",
    "        visual_feature = self.v_fc(visual_feature)\n",
    "        visual_feature = self.dropout(self.relu(visual_feature))\n",
    "\n",
    "        # spatial-channel attention\n",
    "        visual_feature = self.spatial_channel_att(visual_feature, audio_feature)\n",
    "        visual_feature = visual_feature.transpose(1, 0).contiguous()\n",
    "\n",
    "        # audio query\n",
    "        video_key_value_feature = self.video_encoder(visual_feature)\n",
    "        audio_query_output = self.audio_decoder(audio_feature, video_key_value_feature)\n",
    "\n",
    "        # video query\n",
    "        audio_key_value_feature = self.audio_encoder(audio_feature)\n",
    "        video_query_output = self.video_decoder(visual_feature, audio_key_value_feature)\n",
    "        \n",
    "        video_query_output= self.AVInter(video_query_output, audio_query_output)\n",
    "        scores = self.localize_module(video_query_output)\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class supv_main_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(supv_main_model, self).__init__()\n",
    "\n",
    "        self.spatial_channel_att = New_Audio_Guided_Attention().cuda()\n",
    "        self.video_input_dim = 512 \n",
    "        self.video_fc_dim = 512\n",
    "        self.d_model = 256\n",
    "        self.v_fc = nn.Linear(self.video_input_dim, self.video_fc_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.video_encoder = InternalTemporalRelationModule(input_dim=512, d_model=256)\n",
    "        self.video_decoder = CrossModalRelationAttModule(input_dim=512, d_model=256)\n",
    "        self.audio_encoder = InternalTemporalRelationModule(input_dim=128, d_model=256)\n",
    "        self.audio_decoder = CrossModalRelationAttModule(input_dim=128, d_model=256)\n",
    "\n",
    "        self.AVInter = AudioVideoInter(self.d_model, n_head=4, head_dropout=0.1)\n",
    "        self.localize_module = SupvLocalizeModule(self.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, visual_feature, audio_feature):\n",
    "        # [batch, 10, 512]\n",
    "\n",
    "        # optional, we add a FC here to make the model adaptive to different visual features (e.g., VGG ,ResNet)\n",
    "        audio_feature = audio_feature.transpose(1, 0).contiguous()\n",
    "        visual_feature = self.v_fc(visual_feature)\n",
    "        visual_feature = self.dropout(self.relu(visual_feature))\n",
    "        \n",
    "        # spatial-channel attention \n",
    "        visual_feature = self.spatial_channel_att(visual_feature, audio_feature)\n",
    "\n",
    "        # audio-guided needed\n",
    "        visual_feature = visual_feature.transpose(1, 0).contiguous()\n",
    "\n",
    "        # audio query\n",
    "        video_key_value_feature = self.video_encoder(visual_feature)\n",
    "        audio_query_output = self.audio_decoder(audio_feature, video_key_value_feature)\n",
    "\n",
    "        # video query\n",
    "        audio_key_value_feature = self.audio_encoder(audio_feature)\n",
    "        video_query_output = self.video_decoder(visual_feature, audio_key_value_feature)\n",
    "\n",
    "        video_query_output= self.AVInter(video_query_output, audio_query_output)\n",
    "        scores = self.localize_module(video_query_output)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####video to specto\n",
    "\n",
    "import os\n",
    "import moviepy.editor as mp\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def video_to_save_log_mel_spectrogram(video_path, output_folder, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    audio_temp_path = r\"C:\\Users\\karth\\Downloads\\train\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.wav\"\n",
    "    # Extract audio from the video and save it as a temporary WAV file\n",
    "    video_clip = mp.VideoFileClip(video_path)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(audio_temp_path, codec='pcm_s16le', fps=audio_clip.fps)\n",
    "\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_temp_path, sr=None)\n",
    "    os.remove(audio_temp_path)\n",
    "\n",
    "    # Compute the Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "\n",
    "    # Convert to log scale\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Save the log Mel spectrogram as an image file\n",
    "    save_spectrogram_as_image(log_mel_spectrogram, sr=sr, hop_length=hop_length, output_path=os.path.join(output_folder,video_name))\n",
    "\n",
    "    del y, sr, mel_spectrogram, log_mel_spectrogram, audio_clip, video_clip\n",
    "\n",
    "\n",
    "\n",
    "def save_spectrogram_as_image(spectrogram, sr, hop_length, output_path, title=\"Log Mel Spectrogram\"):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(spectrogram, x_axis='time', y_axis='mel', sr=sr, hop_length=hop_length, cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    output_path=output_path+'.png'\n",
    "    plt.savefig(output_path)  # Save the spectrogram as an image file\n",
    "    plt.close()  # Close the plot to prevent displaying it\n",
    "\n",
    "def process_videos_in_directory(input_folder, output_folder):\n",
    "\n",
    "    videos_to_process = [os.path.join(root, file) for root, _, files in os.walk(input_folder) for file in files if file.endswith(\".mp4\")]\n",
    "\n",
    "    for video_path in tqdm(videos_to_process, desc=\"Processing Videos\"):\n",
    "        video_to_save_log_mel_spectrogram(video_path, output_folder)\n",
    "\n",
    "    # for root, dirs, files in os.walk(input_folder):\n",
    "    #     for file in files:\n",
    "    #         if file.endswith(\".mp4\"):\n",
    "    #             video_path = os.path.join(root, file)\n",
    "    #             video_to_save_log_mel_spectrogram(video_path, output_folder)\n",
    "\n",
    "# Example usage\n",
    "input_folder = r\"C:\\Users\\karth\\Downloads\\train\"\n",
    "output_folder = r\"C:\\Users\\karth\\Downloads\\trainspecto\"\n",
    "\n",
    "process_videos_in_directory(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 157, 1024])\n",
      "torch.Size([5, 157, 128])\n",
      "torch.Size([5, 157, 128])\n",
      "tensor([[[-0.3255, -0.7201, -1.3232,  ...,  0.1799, -0.0492,  0.5752],\n",
      "         [-0.3146, -1.2614, -1.0466,  ...,  0.7632, -0.5162,  1.0648],\n",
      "         [-0.8436, -1.3632, -0.0903,  ...,  0.9423, -0.3327,  0.1240],\n",
      "         ...,\n",
      "         [-1.1972,  1.6714,  0.0734,  ..., -0.1596,  0.4987,  0.3687],\n",
      "         [-0.5031,  1.4383, -2.3368,  ..., -1.9084,  0.2783,  0.0790],\n",
      "         [-0.1501, -0.0322, -2.3169,  ...,  0.0730, -0.4764, -0.4420]],\n",
      "\n",
      "        [[-0.3757, -0.5329, -1.3553,  ...,  0.4328, -0.2067,  0.6716],\n",
      "         [-0.1790, -1.2031, -0.9562,  ...,  0.9599, -0.6266,  1.2794],\n",
      "         [-0.0204, -1.3087, -1.1483,  ...,  0.8486, -0.2002,  0.2717],\n",
      "         ...,\n",
      "         [-1.2709,  2.9776, -0.0408,  ..., -0.3034,  0.3227,  0.5243],\n",
      "         [-0.6743,  1.2299, -2.8904,  ..., -1.8598,  0.3340, -0.3133],\n",
      "         [-0.0254,  0.0122, -2.2938,  ...,  0.0894, -1.0829, -0.4157]],\n",
      "\n",
      "        [[-0.0602, -0.6329, -0.2386,  ...,  0.3167, -0.2925,  0.6234],\n",
      "         [-0.2904, -1.3669, -1.3011,  ...,  1.1352, -0.0153,  1.1633],\n",
      "         [-0.5942, -1.1976, -1.5023,  ...,  0.7996, -0.2948, -0.0923],\n",
      "         ...,\n",
      "         [-1.4635,  3.7785, -0.0979,  ..., -0.0585,  0.0179,  0.3113],\n",
      "         [-0.5804,  1.2319, -0.1039,  ..., -1.9204, -0.6265, -1.6258],\n",
      "         [-0.1258, -0.0573, -2.7239,  ...,  0.1934, -0.5391, -0.1258]],\n",
      "\n",
      "        [[-0.1740, -0.5605, -1.3420,  ...,  0.0997, -0.1401,  0.5613],\n",
      "         [-0.0288, -1.3185, -0.9187,  ...,  0.9455, -0.0280,  1.1356],\n",
      "         [-0.7680, -1.2409, -1.3169,  ...,  0.6964, -0.1875,  0.1893],\n",
      "         ...,\n",
      "         [-1.2697,  3.2743, -0.0513,  ..., -0.3153,  0.6104,  0.4724],\n",
      "         [-0.6017,  1.3433, -2.5647,  ..., -1.7747,  0.2873, -0.2813],\n",
      "         [-0.1541, -0.2396, -2.6449,  ...,  0.0582, -0.1253, -0.3638]],\n",
      "\n",
      "        [[-0.3754, -0.4390, -1.7454,  ...,  0.4826, -0.3595,  0.5470],\n",
      "         [-0.2104, -1.4303, -1.2744,  ...,  1.0773, -0.4829,  1.1306],\n",
      "         [-0.8964, -1.2433, -1.0602,  ..., -0.0831, -0.2135, -0.0831],\n",
      "         ...,\n",
      "         [ 0.0282,  2.1676,  0.0282,  ..., -0.4273,  0.0561,  0.2589],\n",
      "         [-1.0282,  0.7790, -2.2225,  ..., -1.5325, -0.7550, -1.2152],\n",
      "         [-0.2122, -0.4518, -4.2579,  ...,  0.1189, -0.4446, -0.1352]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "####audio video simple fusion\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AudioVideoInter(nn.Module):\n",
    "    def __init__(self, d_model, n_head, head_dropout=0.1):\n",
    "        super(AudioVideoInter, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.video_multihead = MultiheadAttention(d_model, num_heads=n_head, dropout=head_dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.affine_video_1 = nn.Linear(1024, 128)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, video_feat, audio_feat):\n",
    "        # video_feat, audio_feat: [10, batch, 256]\n",
    "        video_feat=self.affine_video_1(video_feat)\n",
    "        global_feat = video_feat * audio_feat\n",
    "        memory = torch.cat([audio_feat, video_feat], dim=0)\n",
    "        mid_out = self.video_multihead(global_feat, memory, memory)[0]\n",
    "        output = self.norm1(global_feat + self.dropout(mid_out))\n",
    "\n",
    "        return  output\n",
    "    \n",
    "\n",
    "aud= np.load(r\"C:\\Users\\karth\\Downloads\\audiofeats\\train\\Your.Name.2016__#01-22-20_01-24-05_label_A.npy\")\n",
    "vid=np.load(r\"C:\\Users\\karth\\Downloads\\XDVioDet-master\\XDVioDet-master\\tratrain\\Your.Name.2016__#01-22-20_01-24-05_label_A.npy\")\n",
    "\n",
    "aud = np.repeat(aud[:, np.newaxis, :], 5, axis=1)\n",
    "aud_tensor = torch.from_numpy(aud)\n",
    "vid_tensor = torch.from_numpy(vid)\n",
    "\n",
    "aud_tensor = aud_tensor.permute(1, 0, 2)\n",
    "\n",
    "vid_tensor = vid_tensor.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "print(vid_tensor.shape)\n",
    "print(aud_tensor.shape)\n",
    "\n",
    "model=AudioVideoInter(d_model=128,n_head=2)\n",
    "out=model(vid_tensor,aud_tensor)\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 157, 1024])\n",
      "torch.Size([785, 128])\n",
      "torch.Size([785, 1024])\n",
      "torch.Size([785, 1024])\n",
      "torch.Size([5, 157, 1024, 1])\n",
      "tensor([[[ 0.2326,  1.6311,  0.5422,  ..., -1.2594,  0.3386, -0.7474],\n",
      "         [-0.7677,  1.5403,  1.0024,  ..., -1.2624,  0.8253, -0.6294],\n",
      "         [-0.8642,  0.7435,  0.1040,  ..., -0.4903,  0.1975, -0.4469],\n",
      "         [-0.7709,  1.0024,  1.0777,  ..., -1.0989, -0.5271, -0.7881],\n",
      "         [-0.7686,  1.7643,  0.9162,  ..., -0.9813,  0.8918, -0.5953]],\n",
      "\n",
      "        [[-0.5472,  2.0676,  0.8976,  ..., -0.9731, -0.0681, -0.3853],\n",
      "         [-0.6023,  1.6399,  0.8768,  ..., -1.4973, -0.0995, -0.4158],\n",
      "         [-0.8948,  1.8757,  0.6564,  ..., -1.3329,  0.2155, -1.1678],\n",
      "         [-1.0295, -0.3664,  0.7256,  ..., -1.4664, -0.2354, -0.5229],\n",
      "         [-0.8598,  1.8328,  0.2853,  ..., -1.4059, -0.3516, -1.0934]],\n",
      "\n",
      "        [[-0.1162,  0.7674,  1.9938,  ..., -1.6808,  0.5101, -0.7139],\n",
      "         [-0.6882,  0.9703,  2.1789,  ..., -1.3561,  0.1778, -0.4513],\n",
      "         [-0.4149,  0.9153,  1.5727,  ..., -1.3145, -0.2757, -1.3476],\n",
      "         [-0.2055,  0.7389,  2.0483,  ..., -1.6051,  0.1192, -0.5137],\n",
      "         [-0.3728,  1.8061,  1.8627,  ..., -1.3790,  0.1473, -0.7199]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0116,  0.8841,  0.6164,  ..., -1.1933, -0.2882, -1.6530],\n",
      "         [-1.3257,  1.4579,  1.4587,  ..., -0.9119,  0.4793, -0.8214],\n",
      "         [-1.0560,  0.9616,  1.2595,  ..., -0.5951,  1.4148, -1.6171],\n",
      "         [-1.5925,  0.8086,  1.9824,  ..., -1.1909,  0.2921, -1.6140],\n",
      "         [-1.1537, -0.8036,  1.2492,  ..., -1.0545,  0.9212, -1.5920]],\n",
      "\n",
      "        [[-0.9591,  0.5204,  1.2215,  ..., -0.3135,  0.3591, -1.2972],\n",
      "         [-1.0122,  1.0197,  0.9352,  ..., -1.0492,  1.0104, -1.5859],\n",
      "         [-0.1913,  0.8322,  0.3951,  ..., -0.8015,  0.6938, -1.2835],\n",
      "         [-1.1583, -0.1205,  1.4172,  ..., -0.8776,  0.6908, -1.6390],\n",
      "         [-1.0085,  0.7054,  2.0369,  ..., -0.7086,  0.7862, -1.1463]],\n",
      "\n",
      "        [[ 0.0846,  0.2163,  0.5007,  ..., -0.3827, -0.6424, -0.5246],\n",
      "         [-1.1811,  0.7740,  0.6179,  ..., -0.4997,  0.0503, -0.7753],\n",
      "         [-1.1568,  0.9761,  0.4491,  ..., -1.1093,  0.1031, -0.7620],\n",
      "         [-1.2218,  0.3788,  0.9908,  ..., -0.7197, -0.0516, -0.4346],\n",
      "         [-1.0745,  0.7013,  0.4700,  ..., -0.9477,  0.3377, -0.3473]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([157, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####full combinatio  with crm\n",
    "\n",
    "class weak_main_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(weak_main_model, self).__init__()\n",
    "        self.spatial_channel_att = New_Audio_Guided_Attention()\n",
    "        self.video_input_dim = 1024 \n",
    "        self.video_fc_dim = 1024\n",
    "        self.d_model = 128\n",
    "        self.v_fc = nn.Linear(self.video_input_dim, self.video_fc_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.video_encoder = InternalTemporalRelationModule(input_dim=self.video_fc_dim, d_model=self.d_model)\n",
    "        self.video_decoder = CrossModalRelationAttModule(input_dim=self.video_fc_dim, d_model=self.d_model)\n",
    "        self.audio_encoder = InternalTemporalRelationModule(input_dim=128, d_model=self.d_model)\n",
    "        self.audio_decoder = CrossModalRelationAttModule(input_dim=128, d_model=self.d_model)\n",
    "\n",
    "        self.AVInter = AudioVideoInter(self.d_model, n_head=2, head_dropout=0.1)\n",
    "        self.localize_module = WeaklyLocalizationModule(self.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, visual_feature, audio_feature):\n",
    "        # [batch, 10, 512]\n",
    "        # this fc is optinal, that is used for adaption of different visual features (e.g., vgg, resnet).\n",
    "        audio_feature = audio_feature.transpose(1, 0).contiguous()\n",
    "        visual_feature = self.v_fc(visual_feature)\n",
    "        visual_feature = self.dropout(self.relu(visual_feature))\n",
    "\n",
    "        # spatial-channel attention\n",
    "        visual_feature=visual_feature.unsqueeze(3)\n",
    "        visual_feature = self.spatial_channel_att(visual_feature, audio_feature)\n",
    "        visual_feature = visual_feature.transpose(1, 0).contiguous()\n",
    "\n",
    "        # audio query\n",
    "        video_key_value_feature = self.video_encoder(visual_feature)\n",
    "        audio_query_output = self.audio_decoder(audio_feature, video_key_value_feature)\n",
    "\n",
    "        # video query\n",
    "        audio_key_value_feature = self.audio_encoder(audio_feature)\n",
    "        video_query_output = self.video_decoder(visual_feature, audio_key_value_feature)\n",
    "        \n",
    "        video_query_output= self.AVInter(video_query_output, audio_query_output)\n",
    "        #scores = self.localize_module(video_query_output)\n",
    "\n",
    "        return video_query_output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "aud= np.load(r\"C:\\Users\\karth\\Downloads\\audiofeats\\train\\Your.Name.2016__#01-22-20_01-24-05_label_A.npy\")\n",
    "vid=np.load(r\"C:\\Users\\karth\\Downloads\\XDVioDet-master\\XDVioDet-master\\tratrain\\Your.Name.2016__#01-22-20_01-24-05_label_A.npy\")\n",
    "aud = np.repeat(aud[:, np.newaxis, :], 5, axis=1)\n",
    "# aud = np.repeat(aud[:, :, :], 8, axis=2)\n",
    "aud_tensor = torch.from_numpy(aud)\n",
    "vid_tensor = torch.from_numpy(vid)\n",
    "\n",
    "aud_tensor = aud_tensor.permute(1, 0, 2)\n",
    "\n",
    "vid_tensor = vid_tensor.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "model=weak_main_model()\n",
    "out=model(vid_tensor,aud_tensor)\n",
    "print(out)\n",
    "print(out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
